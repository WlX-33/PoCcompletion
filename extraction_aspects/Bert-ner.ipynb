{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# 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\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.36.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a token classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "model_checkpoint = \"roberta-base\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Features, Sequence\n",
    "\n",
    "\n",
    "def process_text_to_conll(text):\n",
    "    sentences = []\n",
    "    current_sentence = {'tokens': [], 'ner_tags': []}\n",
    "\n",
    "    for line in text.split('\\n'):\n",
    "        if line.strip() == '':\n",
    "            if current_sentence['tokens']:\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = {'tokens': [], 'ner_tags': []}\n",
    "        else:\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                token, ner_tag = parts[0], parts[1]\n",
    "                current_sentence['tokens'].append(token)\n",
    "                current_sentence['ner_tags'].append(ner_tag)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Read content from a text file\n",
    "file_path = 'trainset.txt'\n",
    "raw_text = read_text_file(file_path)\n",
    "\n",
    "# Process text data into CoNLL-2003 format\n",
    "sentences = process_text_to_conll(raw_text)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(sentences))\n",
    "valid_size = int(0.2 * len(sentences))\n",
    "# test_size = len(sentences) - train_size - valid_size\n",
    "\n",
    "# Create a DatasetDict object\n",
    "dataset_dict = DatasetDict()\n",
    "\n",
    "# Create Dataset objects using lists\n",
    "dataset_dict['train'] = Dataset.from_dict({'tokens': [s['tokens'] for s in sentences[:train_size]],\n",
    "                                           'ner_tags': [s['ner_tags'] for s in sentences[:train_size]]})\n",
    "dataset_dict['validation'] = Dataset.from_dict({'tokens': [s['tokens'] for s in sentences[train_size:train_size+valid_size]],\n",
    "                                                'ner_tags': [s['ner_tags'] for s in sentences[train_size:train_size+valid_size]]})\n",
    "# dataset_dict['test'] = Dataset.from_dict({'tokens': [s['tokens'] for s in sentences[-test_size:]],\n",
    "#                                           'ner_tags': [s['ner_tags'] for s in sentences[-test_size:]]})\n",
    "# Here, 'ner_tags' need to be converted to category indices to match ClassLabel requirements\n",
    "label_names = ['O', 'B-title', 'I-title', 'B-author', 'I-author', 'B-time', 'I-time', 'B-plat', 'I-plat','B-version', 'I-version']\n",
    "label_mapping = {label: idx for idx, label in enumerate(label_names)}\n",
    "\n",
    "# Create 'ner_tags' feature using ClassLabel\n",
    "ner_feature = ClassLabel(names=label_names, num_classes=len(label_names))\n",
    "\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Features, Sequence, Value\n",
    "\n",
    "# Define a transformCamScore function that scales 'ner_tags' to card category labels\n",
    "def convert_labels(example):\n",
    "    example['ner_tags'] = [label_mapping[tag] for tag in example['ner_tags']]\n",
    "    return example\n",
    "\n",
    "features = Features({\n",
    "    'tokens': Sequence(Value(dtype=\"string\")),\n",
    "    'ner_tags': Sequence(Value(dtype=\"int64\")),\n",
    "})\n",
    "\n",
    "dataset_dict['train'] = dataset_dict['train'].map(convert_labels, features=features)\n",
    "dataset_dict['validation'] = dataset_dict['validation'].map(convert_labels, features=features)\n",
    "# dataset_dict['test'] = dataset_dict['test'].map(convert_labels, features=features)\n",
    "\n",
    "datasets = dataset_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 61201\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 15300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the training, validation and test sets all have a column for the tokens (the input texts split into words) and one column of labels for each kind of task we introduced before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['##############'], 'ner_tags': [0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are already coded as integer ids to be easily usable by our model, but the correspondence with the actual categories is stored in the `features` of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list = ['O', 'B-title', 'I-title', 'B-author', 'I-author', 'B-time', 'I-time', 'B-plat', 'I-plat','B-version', 'I-version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[127.0.0.1]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[--]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-----]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[&lt;?xml, version=\"1.0\", encoding=\"UTF-8\"?&gt;]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[}]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[*, File, Vuln, checkuser.php]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[injection., One, of, the, default, delegate's, command, is, used, to, handle, https]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[##]</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[#, Including, alignment, of, opcodes, in, memory]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[#15, pc, 000100d1, /system/lib/libutils.so, (_ZN7android6Thread11_threadLoopEPv+112)]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We are now preparing to write a function for preprocessing samples. We provide the \"truncation=True\" parameter (to truncate texts larger than the maximum size allowed by the model) and \"is_split_into_words=True\" (as shown above) to the \"tokenizer.\" Then, we align the labels with the token IDs using the strategy we have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 849, 49727, 49629, 10431, 2], [0, 849, 16161, 21058, 13497, 4832, 9318, 10699, 9017, 41614, 96, 35892, 2], [0, 849, 2], [0, 849, 16161, 21058, 14338, 4832, 287, 3592, 219, 1728, 6282, 2010, 2711, 2], [0, 849, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, -100], [-100, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, -100], [-100, 0, -100], [-100, 0, 0, 0, 0, 0, 3, 3, 3, 3, 4, 4, 4, -100], [-100, 0, -100]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}-{'ARG'}\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require a data collator that will batch processed examples together while applying padding to make them all the same size (each batch will be padded to the length of its longest example). The Transformers library includes a data collator for this task that not only pads the inputs but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification,  EarlyStoppingCallback\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final aspect we define for our \"Trainer\" is how to compute metrics based on predictions. Here, we utilize the [`seq...` metric, loaded via the datasets library, which is commonly used for evaluating results on the CONLL datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric takes list of labels for the predictions and references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='38260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6000/38260 15:03 < 1:20:58, 6.64 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.629996</td>\n",
       "      <td>0.684858</td>\n",
       "      <td>0.448942</td>\n",
       "      <td>0.542356</td>\n",
       "      <td>0.962803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.606480</td>\n",
       "      <td>0.568824</td>\n",
       "      <td>0.466409</td>\n",
       "      <td>0.512551</td>\n",
       "      <td>0.964163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.588600</td>\n",
       "      <td>0.602304</td>\n",
       "      <td>0.619705</td>\n",
       "      <td>0.656030</td>\n",
       "      <td>0.637350</td>\n",
       "      <td>0.968286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.609778</td>\n",
       "      <td>0.608768</td>\n",
       "      <td>0.429123</td>\n",
       "      <td>0.503399</td>\n",
       "      <td>0.964023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.587100</td>\n",
       "      <td>0.595035</td>\n",
       "      <td>0.724974</td>\n",
       "      <td>0.587504</td>\n",
       "      <td>0.649040</td>\n",
       "      <td>0.970490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.601393</td>\n",
       "      <td>0.786508</td>\n",
       "      <td>0.654014</td>\n",
       "      <td>0.714168</td>\n",
       "      <td>0.972371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.599042</td>\n",
       "      <td>0.773484</td>\n",
       "      <td>0.437017</td>\n",
       "      <td>0.558489</td>\n",
       "      <td>0.969036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.713327</td>\n",
       "      <td>0.606819</td>\n",
       "      <td>0.655776</td>\n",
       "      <td>0.970199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.575700</td>\n",
       "      <td>0.591644</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.618576</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>0.971536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.601114</td>\n",
       "      <td>0.709233</td>\n",
       "      <td>0.683742</td>\n",
       "      <td>0.696254</td>\n",
       "      <td>0.968722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.591482</td>\n",
       "      <td>0.705951</td>\n",
       "      <td>0.613705</td>\n",
       "      <td>0.656604</td>\n",
       "      <td>0.969139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.589833</td>\n",
       "      <td>0.738922</td>\n",
       "      <td>0.672153</td>\n",
       "      <td>0.703958</td>\n",
       "      <td>0.971822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.586022933959961, metrics={'train_runtime': 904.0753, 'train_samples_per_second': 676.946, 'train_steps_per_second': 42.319, 'total_flos': 3088773359073066.0, 'train_loss': 0.586022933959961, 'epoch': 1.57})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKASz-2vIrJi"
   },
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOUcBkX8IrJi",
    "outputId": "de5b9dd6-9dc0-4702-cb43-55e9829fde25"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# loaded_model = AutoModelForTokenClassification.from_pretrained('/home/dwj/WY/bert/store_model/roberta-base-finetuned-ner/checkpoint-4000')\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# loaded_model.to(device)\n",
    "# trainer.model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the precision/recall/f1 computed for each category now that we have finished training, we can apply the same function as before on the result of the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets['validation'])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "result_tuples = [\n",
    "    ('author', results['author']['precision'], results['author']['recall'],\n",
    "     results['author']['f1'], results['author']['number']),\n",
    "    ('plat', results['plat']['precision'], results['plat']['recall'],\n",
    "     results['plat']['f1'], results['plat']['number']),\n",
    "    ('time', results['time']['precision'], results['time']['recall'],\n",
    "     results['time']['f1'], results['time']['number']),\n",
    "    ('title', results['title']['precision'], results['title']['recall'],\n",
    "     results['title']['f1'], results['title']['number']),\n",
    "    ('version', results['version']['precision'], results['version']['recall'],\n",
    "     results['version']['f1'], results['version']['number']),\n",
    "    ('overall', results['overall_precision'], results['overall_recall'],\n",
    "     results['overall_f1'], results['author']['number']+results['plat']['number']+results['time']['number']+results['title']['number']+results['version']['number'])  # 'overall' does not have a 'number' field\n",
    "]\n",
    "\n",
    "\n",
    "result_table = tabulate(result_tuples, headers=['Label', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "\n",
    "print(result_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# from transformers import pipeline\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# def read():\n",
    "#     file_path = []\n",
    "#     for file in os.listdir('app_data'):\n",
    "#         if file.endswith('.txt'):\n",
    "#             file_path.append(os.path.join( 'app_data', file))\n",
    "#     return file_path\n",
    "\n",
    "# def extract(filepath_list):\n",
    "\n",
    "#     label_names = ['O', 'B-title', 'I-title', 'B-author', 'I-author', 'B-time', 'I-time','B-reference', 'B-plat', 'I-plat','B-version', 'I-version']\n",
    "#     model_checkpoint = 'bert/store_model/roberta-base-finetuned-ner/checkpoint-38000'\n",
    "\n",
    "#     token_classifier = pipeline(\n",
    "#         \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    "#     )\n",
    "\n",
    "#     sumnum = len(filepath_list)\n",
    "#     countnum = 0\n",
    "#     for filepath in filepath_list:\n",
    "\n",
    "#         countnum += 1\n",
    "#         print(f'processï¼š{countnum}/{sumnum}', end='\\r')\n",
    "\n",
    "#         with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#             file_content = file.read()\n",
    "\n",
    "#         results = token_classifier(file_content)\n",
    "\n",
    "#         for item in results:\n",
    "#             item['entity_group'] = label_names[int(item['entity_group'].split('_')[1])]\n",
    "\n",
    "#         \n",
    "#         final_results = []\n",
    "#         pre_label = \"\"\n",
    "#         for result in results:\n",
    "#             label = result['entity_group']\n",
    "#             if label == 'O':\n",
    "#                 continue\n",
    "#             ordlabel = label.split('-')[0]\n",
    "#             nerlabel = label.split('-')[1]\n",
    "#             if nerlabel !=  pre_label:\n",
    "#                 result['entity_group'] = nerlabel\n",
    "#                 final_results.append(result)\n",
    "#             elif ordlabel == 'I':\n",
    "#                 final_results[-1]['word'] += ' '+result['word']\n",
    "#                 final_results[-1]['end'] = result['end']\n",
    "#             pre_label = nerlabel\n",
    "\n",
    "#         \n",
    "#         for result in final_results:\n",
    "#             for key, value in result.items():\n",
    "#                 if isinstance(value, np.float32):\n",
    "#                     result[key] = value.item()\n",
    "\n",
    "#         directory, filename = os.path.split(filepath)\n",
    "#         print(os.path.join(directory, filename[:-5] + '_POC_aspects.json'))\n",
    "#         with open(os.path.join(directory, filename[:-5] + '_POC_aspects.json'), \"w\") as file:\n",
    "#             json.dump(final_results, file, indent=4)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     filepath_list = read()\n",
    "#     extract(filepath_list)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
